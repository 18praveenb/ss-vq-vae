{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric learning training set preparation\n",
    "\n",
    "This notebook prepares a training set from the Mixing Secrets collection for training the timbre metric. Run `../download.ipynb` first.\n",
    "\n",
    "Copyright 2020 InterDigital R&D and Télécom Paris.  \n",
    "Author: Ondřej Cífka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import concurrent.futures as cf\n",
    "import glob\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import regex as re\n",
    "import shutil\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import essentia\n",
    "import essentia.standard as estd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../download'\n",
    "OUTPUT_DIR = 'wav_16kHz'\n",
    "SR_IN = 44100  # Essentia default\n",
    "SR = 16000\n",
    "MAX_FILES_PER_SONG = 12\n",
    "MAX_SEGMENTS_PER_FILE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_BLACKLIST = [\n",
    "    r'over\\b', 'overhead', 'room'\n",
    "]\n",
    "TRACK_BLACKLIST_RE = re.compile('(' + '|'.join(x for x in TRACK_BLACKLIST) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_track_name(name):\n",
    "    name = re.sub(r'[^\\p{L}]', ' ', name)\n",
    "    name = re.sub(r'(\\p{Ll})(\\p{Lu})', r'\\1 \\2', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    name = name.strip().lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = []\n",
    "excluded_names = set()\n",
    "included_names = set()\n",
    "\n",
    "for song_path in glob.glob(os.path.join(INPUT_DIR, '*')):\n",
    "    # Use song name as seed\n",
    "    seed = os.path.basename(song_path).encode()\n",
    "    seed = int.from_bytes(hashlib.sha512(seed).digest(), 'big')\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    song_audio_paths = []\n",
    "    for path in glob.glob(os.path.join(song_path, '**', '*.*'), recursive=True):\n",
    "        if not os.path.splitext(path)[1] in ['.wav', '.flac']:\n",
    "            continue\n",
    "        name, _ = os.path.splitext(os.path.basename(path))\n",
    "        name = normalize_track_name(name)\n",
    "        if TRACK_BLACKLIST_RE.search(name) or len(name) == 0:\n",
    "            excluded_names.add(name)\n",
    "            continue\n",
    "        included_names.add(name)\n",
    "        song_audio_paths.append(path)\n",
    "    \n",
    "    song_audio_paths.sort()\n",
    "    rng.shuffle(song_audio_paths)\n",
    "    audio_paths.extend(song_audio_paths[:MAX_FILES_PER_SONG])\n",
    "audio_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(path):\n",
    "    # Use filename as seed\n",
    "    seed = os.path.basename(path).encode()\n",
    "    seed = int.from_bytes(hashlib.sha512(seed).digest(), 'big')\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    audio = estd.EasyLoader(filename=path)()\n",
    "    frame_size = SR_IN // 10\n",
    "    silence_alg = estd.SilenceRate(thresholds=[essentia.db2lin(-60 / 2)])\n",
    "    silence = np.array([silence_alg(frame) for frame in \n",
    "                        estd.FrameGenerator(audio, frameSize=frame_size, hopSize=frame_size)]).reshape(-1)\n",
    "    \n",
    "    # Find 8-second segments with < 50% silence and starting with a non-silent frame\n",
    "    n = 8 * SR_IN // frame_size\n",
    "    silence_cumsum = np.pad(np.cumsum(silence), (1, 0))\n",
    "    silence_sums = silence_cumsum[n + 1:-1] - silence_cumsum[1:-n - 1]\n",
    "    [candidates] = np.where((silence_sums < n // 2) & (silence[:-n - 1] == 0))\n",
    "    candidates *= frame_size\n",
    "\n",
    "    rng.shuffle(candidates)\n",
    "    if len(candidates) < 2:\n",
    "        return []\n",
    "    if len(candidates) % 2 == 1:\n",
    "        candidates = candidates[:-1]\n",
    "    candidates = candidates[:MAX_SEGMENTS_PER_FILE]\n",
    "\n",
    "    meta = []\n",
    "    i_len = len(str(MAX_SEGMENTS_PER_FILE - 1))\n",
    "    for i in candidates:\n",
    "        out_path = os.path.join(OUTPUT_DIR,\n",
    "                                os.path.relpath(path, INPUT_DIR).replace(os.path.sep, '_'))\n",
    "        out_path = os.path.splitext(out_path)[0]\n",
    "        out_path = f'{out_path}.{str(i).zfill(i_len)}.wav'\n",
    "        segment_audio = audio[i:i + 8 * SR_IN]\n",
    "        sf.write(out_path,\n",
    "                 estd.Resample(inputSampleRate=SR_IN, outputSampleRate=SR)(segment_audio),\n",
    "                 samplerate=SR)\n",
    "\n",
    "        meta.append({\n",
    "            'path': os.path.relpath(out_path, OUTPUT_DIR),\n",
    "            'track_name': os.path.relpath(path, INPUT_DIR).split(os.path.sep)[0],\n",
    "            'src_path': os.path.relpath(path, INPUT_DIR),\n",
    "            'src_offset': i / SR_IN\n",
    "        })\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cfb5e1bdf64183a2efef2e78fa1694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4874.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR)\n",
    "with cf.ProcessPoolExecutor(16) as pool:\n",
    "    results = [item\n",
    "               for items in tqdm(pool.map(process_file, audio_paths, chunksize=4), total=len(audio_paths))\n",
    "               for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metadata_single.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_file = collections.defaultdict(list)\n",
    "for item in results:\n",
    "    results_by_file[item['src_path']].append(item)\n",
    "results_by_song = collections.defaultdict(list)\n",
    "for item in results:\n",
    "    results_by_song[item['track_name']].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate triplets: anchor, positive example, negative example\n",
    "# Positive examples are from the same file\n",
    "# Negative examples are from different songs\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "with open('triplets', 'w') as f:\n",
    "    for src_path, items in sorted(results_by_file.items()):\n",
    "        name, _ = os.path.splitext(os.path.basename(src_path))\n",
    "        name = normalize_track_name(name)\n",
    "        \n",
    "        items = list(items)\n",
    "        rng.shuffle(items)\n",
    "\n",
    "        for anchor, positive in zip(items[::2], items[1::2]):\n",
    "            negative_track_name = rng.choice([tn for tn in results_by_song if tn != anchor['track_name']])\n",
    "            negative = rng.choice(results_by_song[negative_track_name])\n",
    "            print(*(os.path.join(OUTPUT_DIR, x['path']) for x in [anchor, positive, negative]),\n",
    "                  sep='\\t', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf <triplets >triplets_shuf\n",
    "!head -n -400 triplets_shuf >triplets_train\n",
    "!tail -n 200 triplets_shuf >triplets_test\n",
    "!tail -n 400 triplets_shuf | head -n 200 >triplets_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7781 triplets\r\n",
      "   7781 triplets_shuf\r\n",
      "    200 triplets_test\r\n",
      "   7381 triplets_train\r\n",
      "    200 triplets_val\r\n",
      "  23343 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l triplets*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
